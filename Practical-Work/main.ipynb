{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from dataloader import CustomEncodingVocabulary, GPT2Dataset\n",
    "CustomEncodingVocabulary.initialize()\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from helper import get_next_run_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T09:32:10.763502Z",
     "start_time": "2025-02-06T09:31:58.725174Z"
    }
   },
   "id": "cd6e01fa7f4e0a41",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 838\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the block size (e.g., 1024 tokens for GPT-2)\n",
    "# Set training parameters\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "vocabulary = CustomEncodingVocabulary.tokens\n",
    "padding_token = CustomEncodingVocabulary.padding_token\n",
    "\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(vocabulary),    # Size of your vocabulary (adjust to match your tokenizer)\n",
    "    n_positions=4096,    # Maximum sequence length\n",
    "    n_ctx=1024,          # Context window size\n",
    "    n_embd=768,          # Embedding size\n",
    "    n_layer=12,          # Number of transformer layers\n",
    "    n_head=12,           # Number of attention heads\n",
    "    pad_token_id=padding_token,  # Set padding token ID (e.g., same as eos_token)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T09:32:10.825004Z",
     "start_time": "2025-02-06T09:32:10.766157Z"
    }
   },
   "id": "87af4c202b7c3c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Instantiate GPT-2 model\n",
    "model = GPT2LMHeadModel(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T09:32:16.995539Z",
     "start_time": "2025-02-06T09:32:13.893169Z"
    }
   },
   "id": "414d561771df2289",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get dataset and dataloader\n",
    "dataset = GPT2Dataset('ldp_5_dataset')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,  # Number of samples per batch\n",
    "        shuffle=False,  # This would fuck up our preloading\n",
    "        num_workers=0,  # This would fuck up our preloading as well... \n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T09:32:24.365421Z",
     "start_time": "2025-02-06T09:32:16.997534Z"
    }
   },
   "id": "84999897840c4d5a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: runs\\GPT2_Model_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1875500 [08:05<?, ?it/s]\u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# Get input_ids and labels\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mtensor(batch[\u001B[38;5;241m0\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     31\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(batch[\u001B[38;5;241m1\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# Create the labels which are just the inputs shifted to the right with a padding token at the end\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[6], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# Get input_ids and labels\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mtensor(batch[\u001B[38;5;241m0\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     31\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(batch[\u001B[38;5;241m1\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# Create the labels which are just the inputs shifted to the right with a padding token at the end\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create tensorboard logger in a new folder, so I have everything logged everytime, since I often forget and then it writes multiple runs into one folder which is a pain to separate. \n",
    "# Get the new folder path\n",
    "log_dir = get_next_run_folder('GPT2_Model')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Initialize SummaryWriter with the new log directory\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Logging to: {log_dir}\")\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabulary[-1] + 1)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Get input_ids and labels\n",
    "        input_ids = torch.tensor(batch[0], device=device)\n",
    "        attention_mask = torch.tensor(batch[1], device=device)\n",
    "        # Create the labels which are just the inputs shifted to the right with a padding token at the end\n",
    "        labels = input_ids[:, 1:].clone()  # Drop the first token\n",
    "        padding_tensor = torch.full((len(input_ids), 1), padding_token, device=device, dtype=labels.dtype)\n",
    "        labels = torch.cat([labels, padding_tensor], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss  # GPT-2 directly computes the loss if labels are provided\n",
    "        \n",
    "        # Log the loss\n",
    "        detached_loss = loss.detach().cpu().item()\n",
    "        writer.add_scalar('Loss/train', detached_loss, epoch * len(dataloader) + batch_idx)\n",
    "        total_loss += detached_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    train_loss.append(total_loss)\n",
    "    total_loss = 0\n",
    "\n",
    "print('Training completed!')\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T09:40:58.512762Z",
     "start_time": "2025-02-06T09:40:42.731866Z"
    }
   },
   "id": "5ff3da0bebd28913",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, 'gpt_model.ph')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e26c322b6954a6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
