{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from dataloader import CustomEncodingVocabulary, GPT2Dataset\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from helper import get_next_run_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T00:07:22.885297Z",
     "start_time": "2025-02-06T00:07:11.668724Z"
    }
   },
   "id": "cd6e01fa7f4e0a41",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 838\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the block size (e.g., 1024 tokens for GPT-2)\n",
    "# Set training parameters\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "vocabulary = CustomEncodingVocabulary().tokens\n",
    "padding = CustomEncodingVocabulary().padding\n",
    "\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(vocabulary),    # Size of your vocabulary (adjust to match your tokenizer)\n",
    "    n_positions=4096,    # Maximum sequence length\n",
    "    n_ctx=1024,          # Context window size\n",
    "    n_embd=768,          # Embedding size\n",
    "    n_layer=12,          # Number of transformer layers\n",
    "    n_head=12,           # Number of attention heads\n",
    "    pad_token_id=padding,  # Set padding token ID (e.g., same as eos_token)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T00:07:22.961Z",
     "start_time": "2025-02-06T00:07:22.889284Z"
    }
   },
   "id": "87af4c202b7c3c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Instantiate GPT-2 model\n",
    "model = GPT2LMHeadModel(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T00:07:37.796430Z",
     "start_time": "2025-02-06T00:07:35.334658Z"
    }
   },
   "id": "414d561771df2289",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get dataset and dataloader\n",
    "dataset = GPT2Dataset('ldp_5_dataset')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,  # Number of samples per batch\n",
    "        shuffle=False,  # This would fuck up our preloading\n",
    "        num_workers=0,  # This would fuck up our preloading as well... \n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T00:07:45.255093Z",
     "start_time": "2025-02-06T00:07:37.799422Z"
    }
   },
   "id": "84999897840c4d5a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: runs\\GPT2_Model_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875500 [00:53<?, ?it/s]\n",
      "C:\\Users\\brunner4\\AppData\\Local\\Temp\\ipykernel_18824\\785415930.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(batch[0], device=device)\n",
      "C:\\Users\\brunner4\\AppData\\Local\\Temp\\ipykernel_18824\\785415930.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(batch[1], device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Create the labels which are just the inputs shifted to the right with a padding token at the end\u001B[39;00m\n\u001B[0;32m     34\u001B[0m labels \u001B[38;5;241m=\u001B[39m input_ids[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mclone()  \u001B[38;5;66;03m# Drop the first token\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     38\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39minput_ids, \n\u001B[0;32m     39\u001B[0m                 attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m     40\u001B[0m                 labels\u001B[38;5;241m=\u001B[39mlabels)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Create tensorboard logger in a new folder, so I have everything logged everytime, since I often forget and then it writes multiple runs into one folder which is a pain to separate. \n",
    "# Get the new folder path\n",
    "log_dir = get_next_run_folder('GPT2_Model')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Initialize SummaryWriter with the new log directory\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Logging to: {log_dir}\")\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabulary[-1] + 1)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Get input_ids and labels\n",
    "        input_ids = torch.tensor(batch[0], device=device)\n",
    "        attention_mask = torch.tensor(batch[1], device=device)\n",
    "        # Create the labels which are just the inputs shifted to the right with a padding token at the end\n",
    "        labels = input_ids[:, 1:].clone()  # Drop the first token\n",
    "        ### TODO: We need to concatinate for each sequance a padding token to the end. Currently it does not work but I will fix this tomorrow\n",
    "        labels = torch.cat([labels, torch.tensor([[padding]], device=device, dtype=labels.dtype)], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss  # GPT-2 directly computes the loss if labels are provided\n",
    "        \n",
    "        # Log the loss\n",
    "        detached_loss = loss.detach().cpu().item()\n",
    "        writer.add_scalar('Loss/train', detached_loss, epoch * len(dataloader) + batch_idx)\n",
    "        total_loss += detached_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    train_loss.append(total_loss)\n",
    "    total_loss = 0\n",
    "\n",
    "print('Training completed!')\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T00:10:59.610663Z",
     "start_time": "2025-02-06T00:10:55.257581Z"
    }
   },
   "id": "5ff3da0bebd28913",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, 'gpt_model.ph')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e26c322b6954a6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
