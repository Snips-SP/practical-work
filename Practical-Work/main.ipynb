{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from dataloader import CustomEncodingVocabulary, GPT2Dataset\n",
    "CustomEncodingVocabulary.initialize()\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from helper import get_next_run_folder\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:53:43.475368Z",
     "start_time": "2025-02-06T19:53:43.462065100Z"
    }
   },
   "id": "cd6e01fa7f4e0a41",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 421\n",
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 4\n",
    "\n",
    "vocabulary = CustomEncodingVocabulary.tokens\n",
    "padding_token = CustomEncodingVocabulary.padding_token\n",
    "\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(vocabulary),    # Size of your vocabulary (adjust to match your tokenizer)\n",
    "    n_positions=1024,    # Maximum sequence length\n",
    "    n_ctx=512,          # Context window size\n",
    "    n_embd=768,          # Embedding size\n",
    "    n_layer=6,          # Number of transformer layers\n",
    "    n_head=6,           # Number of attention heads\n",
    "    pad_token_id=padding_token,  # Set padding token ID (e.g., same as eos_token)\n",
    ")\n",
    "\n",
    "# Use appropriate gpu or cpu\n",
    "device = ('xpu' if torch.xpu.is_available() else\n",
    "          'cuda' if torch.cuda.is_available() else\n",
    "          'cpu')\n",
    "\n",
    "print('Using device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:53:43.475368Z",
     "start_time": "2025-02-06T19:53:43.465570600Z"
    }
   },
   "id": "87af4c202b7c3c",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Instantiate GPT-2 model\n",
    "model = GPT2LMHeadModel(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:53:44.207741400Z",
     "start_time": "2025-02-06T19:53:43.469366800Z"
    }
   },
   "id": "414d561771df2289",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get dataset and dataloader\n",
    "dataset = GPT2Dataset('ldp_5_dataset')\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,  # Number of samples per batch\n",
    "        shuffle=False,  # This would fuck up our preloading\n",
    "        num_workers=0,  # This would fuck up our preloading as well... \n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:53:49.068811300Z",
     "start_time": "2025-02-06T19:53:44.207741400Z"
    }
   },
   "id": "84999897840c4d5a",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: runs\\GPT2_Model_31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37510 [00:48<?, ?it/s]\n",
      "  0%|          | 49/75020 [01:41<41:12:40,  1.98s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 51\u001B[0m\n\u001B[0;32m     48\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m detached_loss\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     53\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TorchXPU\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TorchXPU\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TorchXPU\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Create tensorboard logger in a new folder, so I have everything logged everytime, since I often forget and then it writes multiple runs into one folder which is a pain to separate. \n",
    "# Get the new folder path\n",
    "log_dir = get_next_run_folder('GPT2_Model')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Initialize SummaryWriter with the new log directory\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Logging to: {log_dir}\")\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocabulary[-1] + 1)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "# Enable memory optimizations (we can get away with less memory)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Get input_ids and labels\n",
    "        input_ids = batch[0].to(device).long() \n",
    "        attention_mask = batch[1].to(device).long() \n",
    "        # Create the labels which are just the inputs shifted to the right with a padding token at the end\n",
    "        labels = torch.cat([input_ids[:, 1:], torch.full((len(input_ids), 1), padding_token, device=device, dtype=torch.long)], dim=1)\n",
    "        \n",
    "        # Forward pass using half precision to get away with even less memory\n",
    "        with torch.autocast(device_type='xpu', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            loss = outputs.loss  # GPT-2 directly computes the loss if labels are provided\n",
    "        \n",
    "        # Log the loss\n",
    "        detached_loss = loss.detach().cpu().item()\n",
    "        writer.add_scalar('Loss/train', detached_loss, epoch * len(dataloader) + batch_idx)\n",
    "        total_loss += detached_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    train_loss.append(total_loss)\n",
    "    total_loss = 0\n",
    "\n",
    "print('Training completed!')\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:55:32.131581900Z",
     "start_time": "2025-02-06T19:53:49.073812500Z"
    }
   },
   "id": "5ff3da0bebd28913",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, 'gpt_model.ph')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T19:55:32.138584300Z",
     "start_time": "2025-02-06T19:55:32.132582Z"
    }
   },
   "id": "5e26c322b6954a6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
